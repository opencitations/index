import csv
import argparse
from collections import defaultdict
from zipfile import ZipFile
import io
from tqdm import tqdm
import re
import sys
import requests
from time import sleep
import json

from oc.index.utils.logging import get_logger

csv.field_size_limit(sys.maxsize)

'''
Regex to get all the IDs in the Meta CSV dump
'''
def re_get_ids(val, identifiers, multi_ids = False, group_ids= False):
    res = []
    items = [val]
    if multi_ids:
        items = [item for item in val.split("; ")]

    for item in items:
        re_rule = "(.*)"
        if multi_ids:
            re_rule = "\[(.*)\]"

        re_ids_container = re.search(re_rule,item)
        if re_ids_container:
            re_ids = re.findall("(("+"|".join(identifiers)+")\:\S[^\s]+)", re_ids_container.group(1))
            oids = [oid[0] for oid in re_ids]
            if group_ids:
                res.append(oids)
            else:
                for _id in oids:
                    res.append(_id)
    return res

'''
To create the omid map using the original OC META CSV dump
'''
def get_omid_map(fzip, wanted_id):
    omid_map = dict()
    with ZipFile(fzip) as archive:
        logger.info("Total number of files in the archive is:"+str(len(archive.namelist())))
        for csv_name in tqdm(archive.namelist()):
            with archive.open(csv_name) as csv_file:

                l_cits = list(csv.DictReader(io.TextIOWrapper(csv_file)))
                for o_row in l_cits:
                    omid_ids = re_get_ids(o_row["id"],["omid"])
                    if len(omid_ids) > 0:
                        omid = omid_ids[0].replace("omid:","")
                        other_ids = re_get_ids(o_row["id"], ["doi","issn","isbn","pmid","pmcid","url","wikidata","wikipedia","jid","arxiv"])
                        for any_id in other_ids:
                            if any_id.startswith(wanted_id):
                                any_id = any_id.replace(wanted_id+":","")
                                omid_map[omid] = any_id
    return omid_map

'''
To create the omid map using the META BRs index (in CSV)
The META BRs index should be previously generated using 'meta2redis' command
'''
def read_omid_map(f_omidmap, anyid_pref):
    omid_map = dict()
    with open(f_omidmap, mode='r') as file:
        csv_reader = csv.reader(file)
        for row in tqdm(csv_reader):
            if len(row) == 2:  # Ensure there are exactly two columns
                br_omid, anyids = row
                for _id in anyids.split("; "):
                    if _id.startswith(anyid_pref+":"):
                        omid_map["br/"+br_omid] = _id.replace(anyid_pref+":","")
    return omid_map

def main():
    parser = argparse.ArgumentParser(description='Converts the citation count dump of OpenCitations Index based on BR OMIDs to any other ID (e.g., DOI, PMID)')
    parser.add_argument('--citations', required=True, help='Path to the CSV file containing the citation count in the OpenCitations INDEX expressed as OMID: [COUNT] (*Note: generated by citationcount_dump_gen)')
    parser.add_argument('--redisindex', help='Redis DB storing all the citations of opencitations (*Note: generated by meta2redis)')
    parser.add_argument('--metabrs', help='Path to CSV dump containing the index/map of all BR in Meta (OMIDs) (*Note: generated by meta2redis)')
    parser.add_argument('--metacsv', help='Path to the directory containing the ZIP CSV dump of OC Meta')
    parser.add_argument('--id',  default='doi', help='Convert OMID(s) to a given ID')
    parser.add_argument('--out', default='anyid_citation_count.csv', help='Path to the output CSV file')
    args = parser.parse_args()
    logger = get_logger()

    anyid_pref = args.id

    # Build OMID map
    # DICT => { <OMID>: <anyid_pref>:<ANYID> }
    logger.info("Build OMID map ...")
    if args.metabrs:
        omid_map = read_omid_map(args.metabrs, anyid_pref)
    elif args.metacsv:
        omid_map = get_omid_map(args.metacsv, anyid_pref)


    # Redis DB storing OC Index citations
    redis_cits = None
    if args.redisindex:
        redis_cits = redis.Redis(host='localhost', port=6379, db=args.redisindex)

    # Variables to dump
    multi_any_ids = defaultdict(int)
    anyid_citation_count = dict()

    # Convert OMIDs in the citation count dump
    with open(args.citations, mode='r') as input_csvfile:
        for row in tqdm(csv.reader(input_csvfile)):
            omid = row[0]
            if omid.startswith("omid:"):
                omid = omid.replace("omid:","")
            if not omid.startswith("br/"):
                omid = "br/"+omid
            if omid in omid_map:
                any_id = omid_map[omid]
                cits_count = row[1]

                # check in case this any_id was already processed we need to dissambiguate
                if any_id in anyid_citation_count:

                    # get the any_ids of all the citing entities
                    multi_any_ids[any_id] += 1

                    '''
                    if the DB of redis storing the citations of OpenCitations is specified use that
                    otherwise, use APIs to get the citing entities
                    '''
                    if redis_cits:
                        logger.info("Get citations form Redis for: "+str(anyid_pref+":"+any_id))
                        citing_omids = json.loads(redis_cits.get(omid).decode('utf-8'))
                        citing_dois = set( [omid_map["br/"+__c] for __c in citing_omids] )
                        cits_count = len(set(citing_dois))

                    else:
                        logger.info("Get citations via API for: "+str(anyid_pref+":"+any_id))
                        try:

                            # call META triplestore on test.opencitations.net and get list of citations
                            url = 'https://opencitations.net/index/api/v2/citations/'+anyid_pref+":"+any_id
                            response = requests.get(url)

                            l_citing = [set(cit["citing"].split(" ")) for cit in response.json()]
                            # filter only any_id
                            citings_any_id = set()
                            for citing_obj in l_citing:
                              for k_citing in citing_obj:
                                if k_citing.startswith(anyid_pref+":"):
                                  citings_any_id.add(k_citing.replace(anyid_pref+":",""))

                            cits_count = len(citings_any_id)

                            sleep(1)
                        except:
                            pass

                anyid_citation_count[any_id] = cits_count


    # dump anyid - citation count
    logger.info('Saving '+args.out+', storing the citation counts of '+anyid_pref+' BRs ...')
    with open(args.out, mode='w', newline='') as output_csvfile:
        writer = csv.writer(output_csvfile)
        writer.writerow([anyid_pref, 'citation_count'])
        for c in [(k,anyid_citation_count[k]) for k in anyid_citation_count]:
            writer.writerow([c[0],str(c[1])])

    # dump duplicates
    logger.info('Saving duplicated BR entites ...')
    with open(anyid_pref+"_dupilcates.csv", mode='w', newline='') as output_csvfile:
        writer = csv.writer(output_csvfile)
        writer.writerow([anyid_pref, 'num_duplicates'])
        for c in [ (any_id,multi_any_ids[any_id]) for any_id in multi_any_ids]:
            writer.writerow([c[0],str(c[1])])


#if __name__ == "__main__":
#    main()
