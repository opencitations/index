#!python
# Copyright (c) 2023 Ivan Heibi.
#
# Permission to use, copy, modify, and/or distribute this software for any purpose
# with or without fee is hereby granted, provided that the above copyright notice
# and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH
# REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND
# FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT,
# OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE,
# DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS
# ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS
# SOFTWARE.

import csv
import argparse
from collections import defaultdict
from tqdm import tqdm
import re
import sys
import requests
from time import sleep
import json
import redis

# from zipfile import ZipFile
# import io

from oc.index.utils.logging import get_logger
from oc.index.utils.config import get_config

csv.field_size_limit(sys.maxsize)
config = get_config()


# '''
# Regex to get all the IDs in the Meta CSV dump
# '''
# def re_get_ids(val, identifiers, multi_ids = False, group_ids= False):
#     res = []
#     items = [val]
#     if multi_ids:
#         items = [item for item in val.split("; ")]
#
#     for item in items:
#         re_rule = "(.*)"
#         if multi_ids:
#             re_rule = "\[(.*)\]"
#
#         re_ids_container = re.search(re_rule,item)
#         if re_ids_container:
#             re_ids = re.findall("(("+"|".join(identifiers)+")\:\S[^\s]+)", re_ids_container.group(1))
#             oids = [oid[0] for oid in re_ids]
#             if group_ids:
#                 res.append(oids)
#             else:
#                 for _id in oids:
#                     res.append(_id)
#     return res
#
# '''
# To create the omid map using the original OC META CSV dump
# '''
# def get_omid_map(fzip):
#     global conf_br_ids
#
#     omid_map = dict()
#     with ZipFile(fzip) as archive:
#         logger.info("Total number of files in the archive is:"+str(len(archive.namelist())))
#         for csv_name in tqdm(archive.namelist()):
#             with archive.open(csv_name) as csv_file:
#
#                 l_cits = list(csv.DictReader(io.TextIOWrapper(csv_file)))
#                 for o_row in l_cits:
#                     omid_ids = re_get_ids(o_row["id"],["omid"])
#                     if len(omid_ids) > 0:
#                         omid = omid_ids[0].replace("omid:","")
#                         other_ids = re_get_ids(o_row["id"], conf_br_ids)
#                         omid_map[omid] = set(other_ids)
#     return omid_map

'''
To create the omid map using the META BRs index (in CSV)
The META BRs index should be previously generated using 'meta2redis' command
'''
def read_omid_map(f_omidmap):
    global config

    conf_br_ids = config.get("cnc", "br_ids").split(",")

    omid_map = defaultdict(set)
    with open(f_omidmap, mode='r') as file:
        csv_reader = csv.reader(file)
        for row in csv_reader:
            if len(row) == 2:  # Ensure there are exactly two columns
                br_omid, anyids = row
                br_omid = "br/"+br_omid
                for _id in anyids.split("; "):
                    for anyid_pref in conf_br_ids:
                        if _id.startswith(anyid_pref):
                            omid_map[br_omid].add( _id )

    return omid_map

'''
PARAMS:
    + cited_omid: the cited OMID
    + citing_omids: the list of OMIDs citing *cited_omid*
    + omid_map: the map of OMID > ANYIDs
RETURNS:
    The unique number of citation count
'''
def count_unique_cits(cited_omid, citing_omids, omid_map):

    # Get the ANYIDs of the citing OMIDS
    # Count the unique ones
    unique_brs_anyid = []
    for a_citing_omid in citing_omids:
        _c_intersection = 0
        s_citing_anyids = omid_map[a_citing_omid]
        for __unique in unique_brs_anyid:
            _c_intersection += len(__unique.intersection(s_citing_anyids))
        # if there is no common anyids with the other br entities
        if _c_intersection == 0:
            unique_brs_anyid.append(s_citing_anyids)

    return len(unique_brs_anyid)


def main():

    parser = argparse.ArgumentParser(description='Converts the citation count dump of OpenCitations Index based on BR OMIDs to any other ID (e.g., DOI, PMID)')
    parser.add_argument('--redisindex', required=True, help='Redis DB storing all the citations of opencitations (*Note: populated by cits2redis)')
    parser.add_argument('--metabrs', required=True, help='Path to CSV dump containing the index/map of all BR in Meta (OMIDs) (*Note: generated by meta2redis)')
    parser.add_argument('--id',  default='doi', help='Convert OMID(s) to a given ID')
    parser.add_argument('--out', default='./', help='Path to the output destination dir')
    args = parser.parse_args()
    logger = get_logger()

    # get ANYID Prefix. E:G. "doi"
    anyid_pref = args.id

    # Build OMID map
    # FORMAT:
    #   <OMID>: SET(<anyid_pref>:<anyid_val>)
    # EXAMPLE:
    #   06804319241: {"doi:10.17615/3my7-0q76","pmid:18650518","pmid:18650512","doi:10.1056/nejme0804289","doi:10.1056/nejmoa0708975"}
    logger.info("Build OMID BR map ...")
    omid_map = read_omid_map(args.metabrs)

    # Redis DB storing OC Index citations
    # FORMAT:
    #   <CITED-OMID>:[ <CITING-OMID-1>, <CITING-OMID-2>, ..., <CITING-OMID-N> ]
    # EXAMPLE:
    #   "06503922554": [\"062403812713\", \"0610298487\", \"06250181491\"]
    redis_cits = redis.Redis(host='localhost', port=6379, db=args.redisindex)

    # Results to dump at the end
    anyid_citation_count = defaultdict(set)
    anyid_map = defaultdict(set)

    # (1) Walk through all OMIDS in the OMID map (i.e. omid_map)
    logger.info("Walk through all OMIDS in the OMID map (i.e. omid_map) ...")
    for cited_omid in tqdm(omid_map):

        # Get all citing OMIDs of a given OMID
        citing_omids = set( json.loads(redis_cits.get( cited_omid ).decode('utf-8')) )

        # Get the citation count of *cited_omid*
        citation_count = count_unique_cits(cited_omid, citing_omids, omid_map)

        # Save the citation count for each ANYID of the cited OMID if it starts with prefix needed (e.g., "doi")
        # and update the ANYID map, to monitor the number of OMIDs for each ANYID
        for _anyid in omid_map[cited_omid]:
            if _anyid.startswith(anyid_pref):
                anyid_map[_anyid].add(cited_omid)
                anyid_citation_count[_anyid] = citation_count


    # (2) Work with ANYIDs that have multiple OMIDs
    # Filter the anyid_map previously created to include only ANYIDs that have multi OMIDs
    multi_any_ids = {_anyid:anyids_map[_anyid] for _anyid in anyid_map if len(anyid_map[_anyid]) > 1}

    # Walk throuh this new DICT multi_any_ids
    logger.info("Walk through all ANYIDs that have multiple OMIDs ...")
    for _anyid in tqdm(multi_any_ids):

        # Create a set with all citing OMIDs
        citing_omids = set()
        for cited_omid in multi_any_ids[_anyid]:
            citing_omids.union( set( json.loads(redis_cits.get( cited_omid ).decode('utf-8')) ) )

        # Get the citation count of *cited_omid*
        citation_count = count_unique_cits(cited_omid, citing_omids, omid_map)

        # update the numer of citations of the ANYIDs with multi OMIDs with the new citation count
        anyid_citation_count[_anyid] = citation_count


    # dump anyid - citation count
    logger.info('Saving the citation counts of '+anyid_pref+' BRs ...')
    with open(args.out+anyid_pref+"_citation_count.csv", mode='w', newline='') as output_csvfile:
        writer = csv.writer(output_csvfile)
        writer.writerow([anyid_pref, 'citation_count'])
        for c in [(k,anyid_citation_count[k]) for k in anyid_citation_count]:
            writer.writerow([c[0],str(c[1])])

    # dump duplicates
    logger.info('Saving duplicated BR entites ...')
    with open(args.out+anyid_pref+"_dupilcates.csv", mode='w', newline='') as output_csvfile:
        writer = csv.writer(output_csvfile)
        writer.writerow([anyid_pref, 'num_duplicates'])
        for c in [ (any_id, len(multi_any_ids[any_id]) ) for any_id in multi_any_ids]:
            writer.writerow([c[0],str(c[1])])


#if __name__ == "__main__":
#    main()
